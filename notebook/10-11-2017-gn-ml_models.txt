
%matplotlib inline
import pandas as pd, numpy as np
import sys, os

import sklearn.preprocessing as pp
import sklearn.decomposition as decomposition
import sklearn.model_selection as ms
import sklearn.metrics as metrics

import sklearn.tree as tree
import sklearn.svm as svm
import sklearn.ensemble as ensemble
import sklearn.neighbors as neighbors
# import xgboost
# import keras
sys.path.append('../')
# import lib.tools
pd.options.display.max_columns = 50

train_path = '../bdc/data/train/'



def season(df):
    season_col = df['mois']
    season_col = season_col.replace([12, 1, 2], 0)
    season_col = season_col.replace([3, 4, 5], 1)
    season_col = season_col.replace([6, 7, 8], 2)
    season_col = season_col.replace([9, 10, 11], 3)
    ohc = np.eye(4)[season_col]
    return pd.concat([df, ohc], axis=1)

# for file_ in os.listdir(train_path):
file_ = pd.read_csv(train_path + 'train_1.csv', header=0, delimiter=';',decimal=',',
                    parse_dates=['date'], index_col='date')

file_.dropna(how='any')

#Transform to dummy
dummies = ['ddH10_rose4', 'insee', 'mois']
for dummy in dummies:
    df_dummy = pd.get_dummies(file_[dummy], prefix= dummy)
    file_ = pd.concat([file_, df_dummy], axis=1)
file_.drop(['ddH10_rose4'], inplace=True, axis=1)
file_.drop(['mois'], inplace=True, axis=1)

# Add temporal features
file_['week'] = file_.index.week # WEEK AS CATEGORY ?
# file_.mois = file_.mois.astype('str')
file_ = season(file_, file_['season']) # add season 

# Add lag operator (shift ce fait par ville)
groupby_cities = file_.groupby('insee')
shift = groupby_cities.tH2_obs.shift(1)
file_['tH2_obs_shift_1'] = shift

# file_.drop(['insee'], inplace=True, axis=1)

file_.describe()

file_.head(10)

fa = decomposition.FactorAnalysis()
fa.fit(file_.drop('tH2_obs'))



seed = 1
results = []
names = []
predictions = [] #table of prediction for each model

models = []
models.append(('CART', tree.DecisionTreeRegressor(n_jobs=3)))
models.append(('RF', ensemble.RandomForestRegressor(n_jobs=3)))
models.append(('KNN', neighbors.KNeighborsRegressor(n_jobs=3)))
models.append(('SVM', svm.SVR(n_jobs=3)))
models.append(('GB', ensemble.GradientBoostingRegressor(n_jobs=3)))
# models.append(('NB'), ) # Naive Bayes for modeling uncertainty
# LSTM

first_train = True
for i in range(1,36):
    train_file = train_path + 'train_' + str(i) + '.csv'
    test_file = train_path + 'train_' + str(i+1) + '.csv'
    
    x_test = pd.read_csv(test_file, header=0, delimiter=';',decimal=',',
                    parse_dates=['date'], index_col='date').drop('tH2_obs', axis=1)
    y_test = pd.read_csv(test_file, header=0, delimiter=';',decimal=',',
                    parse_dates=['date'], index_col='date', usecols=['tH2_obs'])
    train = pd.read_csv(train_file, header=0, delimiter=';',decimal=',',
                    parse_dates=['date'], index_col='date')
    
    
    if first_train is True:
        x_train = train.drop('tH2_obs', axis=1)
        y_train = train.pop('tH2_obs')
        first_train = False
    else:
        x_train = pd.concat([x_train, train.drop('tH2_obs', axis=1)])
        y_train = pd.concat([y_train, train.pop('tH2_obs')])
        
    for name, m in models: 
    pred = ms.cross_val_predict(m, dff[features], dff[label], cv=kfold, n_jobs=3)
    cv_res = ms.cross_val_score(m, dff[features], dff[label], cv=kfold, n_jobs=3, scoring = 'accuracy')
    m.fit
    
    predictions.append(pred)
    results.append(cv_res)
    names.append(name)
    print "Score %s: %.3f%%, %.3f%%" % (name, cv_res.mean()*100, cv_res.std()*100)





groupby_cities = file_.groupby('insee')

pd.DataFrame(groupby_cities.tH2_obs.rolling(window=2).mean())

groupby_city.capeinsSOL0.apply(pd.Series.interpolate)


